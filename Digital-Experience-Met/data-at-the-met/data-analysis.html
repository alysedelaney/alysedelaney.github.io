<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <!--PAGE TITLE and DESCRIPTION-->
    <title>Data at the Met: Analysis</title>
    <meta name="Alyse Analysis" content="">
    <!--LINK CSS-->
    <link rel="stylesheet" type="text/css" href="../global-style.css">
    <!-- FONTS -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Alegreya+Sans:wght@300;400;500&family=Alegreya:ital@0;1&family=Archivo:wght@100;400;500;600&family=DM+Serif+Text:ital@0;1&display=swap" rel="stylesheet">
    <!--FAVICON-->
    <link rel="icon" type="image/png" href="assets/favicon.png">
</head>
<body>
    <header>
        <nav class="main-navigation">
            <ul>
                <li><a href="../index.html">HOME</a></li>
                <li><a href="../about.html">ABOUT US</a></li>
                <li><a href="../conclusion.html">CONCLUSION</a></li>
                <li><a href="../references.html">REFERENCES</a></li>
            </ul>
        </nav>
    </header>
   <main>
    <div class="title">
        <h1>Data at the Met</h1>
      </div>
        <div class="chapter-content">
            <nav class="side-navigation">
                <ul>
                    <li><a href="data-introduction.html">INTRO</a></li>
                    <li><a href="data-description.html">DESCRIPTION</a></li>
                    <li><a href="data-analysis.html" id="active-navigation">ANALYSIS</a></li>
                </ul>
            </nav>
            <div class="text">
                <h2>Analysis: The Impact and Future of the Met Museum API</h2>
                <h4 class="byline">by Alyse Delaney, December 2022</h4>
                <p>In today’s hyper-connected digital sphere, a website unlinked to the wider web has metaphorical walls much like the physical gallery space. Information may be buried deep within pages or databases and require substantial digging on behalf of the visitor to find the information they want. The largest impact of the Met’s Open Access program is in the reach their collection now has beyond the museum’s website. The API has been utilized by students to create <a class="body-link" href="https://parsons.nyc/met-museum-2019/" target="_blank">visualizations</a> revealing insights about the collection makeup and by computer scientists developing <a class="body-link" href="https://www.metmuseum.org/blogs/now-at-the-met/2020/digital-salon-artificial-intelligence-open-access" target="_blank">artificial intelligence</a> systems to automatically tag works of art with relevant keywords. Through third-party applications the public is now able to more easily search and explore the Met’s collection. These applications can function as educational tools, meant to teach viewers about art historical periods; as inspirational tools, meant to illuminate creative connections between works in the collection; and as calls for action, meant to reveal shortcomings and potentials in the development of the museum’s collection.</p>
                <h3>Linking Data to the Outside World</h3>
                <p>When the program launched, the Met formed partnerships with institutions like Google, <a class="body-link" href="https://commons.wikimedia.org/wiki/Main_Page" target="_blank">Wikimedia</a>, <a class="body-link" href="https://www.artstor.org/collection/metropolitan-museum-art/" target="_blank">Artstor</a>, and the <a class="body-link" href="https://dp.la/" target="_blank">Digital Public Library of America</a>. Each of these partnerships extend the collection into the world wide web. The Met’s primary partnership with Google is in its contribution to the <a class="body-link" href="https://artsandculture.google.com/partner/the-metropolitan-museum-of-art" target="_blank">Google Arts & Culture</a> project. Once the API was developed, the museum was able to automatically deposit 200,000+ works of art into the cross-institutional repository. Previously, the museum staff had to enter the records by hand and had only completed 757 records.* Partnering with Google also means that the metadata about works of art is searchable via the <a class="body-link" href="https://www.blog.google/products/search/introducing-knowledge-graph-things-not/" target="_blank">Google Knowledge Graph</a>, which forms the at-a-glance summary box frequently displayed at the top of a page when you perform a Google search. Now, if a user searches for “Courbet Woman in the Waves” they can get immediate information about the painting, with links to Wikipedia, the Met, and related artworks in other museums’ collections. Unfortunately, this tool seems to work mostly for prominent Western artists and works of art. A search for “The Feast of Sada”, a folio from the notable manuscript, Shahnama (Book of Kings), which is marked as a highlight of the Islamic Art collection, provides a brief summary but no such blurb on Google despite an extensive <a class="body-link" href="https://www.wikidata.org/wiki/Q29385197" target="_blank">Wikidata</a> entry on the work.</p>
                <p class="data-citation">*Scaling the Mission, Tallon</p>
                <div class="data-analysis-gallery">
                    <img class="data-analysis-image" src="../assets/google-waves.png"> 
                    <a href="https://www.metmuseum.org/art/collection/search/452111" target="blank"><img class="data-analysis-image" src="../assets/feast-of-sada.jpeg"></a>
                    <img class="data-analysis-image" src="../assets/google-feast.png">
                    <div class="data-gallery-caption">
                        <p>Left (Figure 3): Screenshot of the Google search results for "Courbet Woman in the Waves".<br> Middle (Figure 4): "The Feast of Sada", Folio 22v from the Shahnama (Book of Kings) of Shah Tahmasp, ca. 1525, view at <a class="body-link" href="https://www.metmuseum.org/art/collection/search/452111" target="blank">metmuseum.org</a><br>Right (Figure 5): Screenshot of Google search results for "The Feast of Sada Shahnama"</p>
                    </div>
                </div>
                <p>Google’s Knowledge Graph is one instance of the growing trend to connect information online via <a class="body-link" href="https://programminghistorian.org/en/lessons/intro-to-linked-data" target="_blank">linked open data</a>, and it is a movement picking up in cultural institutions to connect their resources to others’. By uploading their data into Wikimedia (the underlying repository of Wikipedia), the Met has contributed knowledge about their collection into the global knowledge base.* Its data is now semantically connected to other pieces of data on the web. These links can be identified in the metadata records provided by the API. For instance, in the record for Courbet’s Woman in the Waves, the artist’s identifier in the Getty Union List of Artist Names is included: </p>
                <p class="data-citation">*via <a class="body-link" href="https://www.metmuseum.org/blogs/now-at-the-met/2018/open-access-at-the-met-year-one" target="_blank">Creating Access beyond metmuseum.org: The Met Collection on Wikipedia</a> Loic Tallon</p>
                <p class="code-block">"constituents": [<br>
                    {<br>
                        "constituentID": 161790,<br>
                        "role": "Artist",<br>
                        "name": "Gustave Courbet",<br>
                        "constituentULAN_URL": "http://vocab.getty.edu/page/ulan/500010927",<br>
                        "constituentWikidata_URL": "https://www.wikidata.org/wiki/Q34618",<br>
                        "gender": ""<br>
                    }<br>
                ],</p>
                <p>And is tagged with the (somewhat disappointing and reductive…) keyword, “Female Nudes,” pulled from the Art & Architecture Thesaurus: </p>
                <p class="code-block">"tags": [<br>
                    {<br>
                        "term": "Female Nudes",<br>
                        "AAT_URL": "http://vocab.getty.edu/page/aat/300189568",<br>
                        "Wikidata_URL": "https://www.wikidata.org/wiki/Q40446"<br>
                    }<br>
                ],</p>
                <p>And the artwork as a whole also has a Wikidata tag:</p>
                <p class="code-block"> "objectWikidata_URL": "https://www.wikidata.org/wiki/Q39013",</p>
                <p>These links to controlled vocabularies and Wikidata URLs essentially make sure every institution is talking about the same instance of an artist, subject, medium, etc.. In turn, linking the item to these instances allows it to be connected to larger ideas and related concepts. For instance, the Met’s website may not explicitly say what movement Courbet belonged to (<a class="body-link" href="https://www.wikidata.org/wiki/Q2642826" target="_blank">Realism</a>) but the Wikidata URL ultimately situates the artwork within a networked field of knowledge and provides links to further explore the topic via Wikipedia. This has tremendous reach in terms of bringing the collection as an educational tool to the wider public, as most search engines use Wikipedia and its underlying databases as the <a class="body-link" href="https://www.youtube.com/watch?v=fUNVgF-VZa8" target="_blank">first point of access </a>when fulfilling queries. Museum staff observe that users will have a deeper engagement with the works via the museum’s website, but the existence of the data via Wikipedia provides crucial access points to the information.*</p>
                <p class="data-citation">*Scaling the Mission, Tallon</p>
                <h3>Next Steps: A Call for Intuitive, Inclusive Searching</h3>
                <p>Overall, while the API is an amazing tool developed by the Metropolitan Museum of Art to expand the reach of its collection, there is still much room for development in providing visitors with quality engagement in terms of information retrieval. Most notably, this could be done through improvement of the search tool in the digital collection.</p>
                <p>When browsing without a particular search in mind, the current filters are difficult to understand for anyone not familiar with art historical language or the collection’s design. Under “Object Type / Material” what is the difference between “Albumen” and “Albumen silver prints”? If a user is trying to identify African art objects, how will they know that these objects actually actually, suscipiciously belong to the “The Michael C. Rockefeller Wing” without any further indication? Furthermore, keyword searches frequently fail to provide accurate results. Why, for instance, when searching for “women artists” in the department of European Paintings, are only 8 of the 40 artworks displayed on the first page of results actually painted by women? Is this a <a class="body-link" href="https://www.metmuseum.org/art/collection/search/849438" target="_blank">shortcoming</a> of the collection itself, or a shortcoming to adequately convey the scope of the collection to their users?</p>
                <div class="data-analysis-gallery">
                    <img class="data-image-full-width" src="../assets/search-results.png">
                    <div class="data-gallery-caption">
                        <p>(Figure 6) Screenshot of the search "women artists" performed in the Met's <a class="body-link" href="https://www.metmuseum.org/art/the-collection" target="_blank">Digital Collection</a></p>
                    </div>
                </div>
                <p>While curious visitors may now be able to answer many of these questions through Google and the broader internet, the museum’s website should still provide comprehensive access for those looking to dive deeper into the content. Unfortunately, intuitive searching typically depends on keywords linked to artworks manually by museum staff. There have been initiatives by the Met to <a class="body-link" href="https://www.metmuseum.org/about-the-met/policies-and-documents/open-access/tagging-initiative" target="_blank">crowdsource tagging</a>, and, as previously mentioned, computer scientists are currently using the API to train software to automatically classify artworks.* Hopefully, this will expedite the process and relieve the labor intensive work of human catalogers. Possibly, a simpler adjustment could be made by narrowing down the filters, or by providing broader categories. Perhaps it would be more intuitive to have a filter for artwork form (Painting, Sculpture, Drawing), and then narrow down into materials. This information is easily connected to the digital collection via the API. For instance, the museum is currently working to include gender information about artists in the metadata, which in turn should ideally populate the search for “women artists.” Ultimately, utilizing linked data and semantic language has the potential to make searching more intuitive for the average, everyday visitor.  Hopefully, the API will provide the museum with the ability to analyze its own shortcomings and provide greater discovery and exploration opportunities for its visitors. </p>
                <p class="data-citation">*via <a class="body-link" href="https://www.metmuseum.org/blogs/collection-insights/2020/met-api-computer-learning" target="_blank">Engaging the Data Science Community with Met Open Access API</a>, Jennie Choi</p>
                <p class="previous-page">&#8592; Previous: <a class="body-link" href="data-description.html">Accessing the Data</a> </p>
            </div>
        </div>
    </main>
   <footer>
    <div class="section-divider"></div>
    <nav class="bottom-navigation">
        <ul>
            <li><a href="../mediated-experience/mediated-introduction.html">MEDIATED EXPERIENCE</a></li>
            <li><a href="../beyond-the-museum/beyond-introduction.html">BEYOND THE MUSEUM</a></li>
            <li><a href="data-introduction.html" class="active-navigation">DATA AT THE MET</a></li>
        </ul>
    </nav>
    <div class="copyright">
        <p>© Alyse Delaney 2022</p>
    </div>
   </footer>
</body>
</html>
